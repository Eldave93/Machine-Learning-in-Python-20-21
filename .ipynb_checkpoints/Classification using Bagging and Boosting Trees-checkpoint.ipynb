{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Day0-Gzd2yzL",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['animation.embed_limit'] = 30000000.0\n",
    "plt.rcParams['figure.dpi'] = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UBi3Ju-i9Yf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Trees\n",
    "\n",
    "Tree-based classifiers are inherently multiclass\n",
    "\n",
    "A decision tree breaks data down by asking a series of questions in order to categorise samples into the same class. An algorithm starts at a tree root and then splits the data based on the features that gives the largest information gain. This splitting procedure occours until all the samples within a given node all belong to the same class. A limit on nodes, or tree depth, is often set to avoid overfitting due to a deep tree. To split using information gain relies on calculating the difference between an impurity measure of a parent node and the sum of the impurities of its child nodes; information gain being high when impurity of the child nodes is low. Three impurity measures that are commonly used in binary decision trees are gini impurity, entropy, and the classification error. An ensemble of decision trees can be created to build a more robust model by giving each tree a random bootstrap sample of the data and using a majority voting rule to predict class label<sup>1</sup>. \n",
    "\n",
    "Lets start by building a tree with one split.\n",
    "\n",
    "---\n",
    "\n",
    "1. Raschka, Sebastian, and Vahid Mirjalili. Python Machine Learning, 2nd Ed. Packt Publishing, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier(criterion='gini',\n",
    "                            max_depth = 1,\n",
    "                            random_state=RANDOM_STATE)\n",
    "\n",
    "DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at our dataset with two feature and one split.\n",
    "\n",
    "Scikit-Learn uses the Classification And Regression Tree (CART) algorithm to produce binary trees, meaning nodes always have two children. Other algorithms, such as ID3, can have more children. The algorithms spits the training set into two subsets using a single feature and a theshold searching for the pair that produces the 'purest' subset based on size, minimising a cost function. Once split, it uses the same logic recursively until the maximum depth is reached or a split cannot be found that reduces impurity<sup>1</sup>. \n",
    "\n",
    "---\n",
    "1. Géron, A. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems. \" O'Reilly Media, Inc.\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from pydotplus import graph_from_dot_data\n",
    "import graphviz\n",
    "\n",
    "#DT.fit(vis_data, y_train)\n",
    "#dot_data = export_graphviz(DT, out_file=None, \n",
    "#                     feature_names=[x_axis_label, y_axis_label],  \n",
    "#                     class_names=feature_reduced['class'].unique(),  \n",
    "#                     filled=True, rounded=True,  \n",
    "#                     special_characters=True)  \n",
    "#\n",
    "# Save it\n",
    "#graph = graph_from_dot_data(dot_data)\n",
    "#graph.write_png('binary_split.png')\n",
    "#\n",
    "# Show it\n",
    "#graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above it has chosen a value at which to split the data up. If below or equal to 0.2 it is classed as seizure and above is baseline.\n",
    "\n",
    "If we then look at the descision boundary we see this is just a straight line representing this split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_decision_regions(vis_data,\n",
    "#                      y_train,\n",
    "#                      clf = DT)\n",
    "#\n",
    "#plt.xlabel(x_axis_label) \n",
    "#plt.ylabel(y_axis_label)\n",
    "#plt.xlim(0,.6)\n",
    "#plt.ylim(0,1.)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now not restrict the number of splits it uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT = DecisionTreeClassifier(criterion='gini',\n",
    "#                            max_depth = None,\n",
    "#                            random_state=RANDOM_STATE)\n",
    "#DT.fit(vis_data, y_train)\n",
    "#\n",
    "#dot_data = export_graphviz(DT, out_file=None, \n",
    "#                     feature_names=[x_axis_label, y_axis_label],  \n",
    "#                     class_names=feature_reduced['class'].unique(),  \n",
    "#                     filled=True, rounded=True,  \n",
    "#                     special_characters=True)  \n",
    "#\n",
    "# Save it\n",
    "#graph = graph_from_dot_data(dot_data)\n",
    "#graph.write_png('multi_split.png')\n",
    "#\n",
    "# Show it\n",
    "#graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen by the descision boundary, a decision tree is quite boxy. Furthermore, how the model makes a decision boundary is going to be affected by the rotation of the data (as DTs create straight lines).\n",
    "\n",
    "In this case, it appears to be overfitting to the training data; see the thin orange line in the blue, this likely wouldnt help in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now fit it on the full training data and see what features it uses to split the data. Decision trees are useful as they allow us assess the importance of each feature to classify the data (more on that later). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT = DecisionTreeClassifier(criterion='gini',\n",
    "#                            max_depth = None,\n",
    "#                            random_state=RANDOM_STATE)\n",
    "#DT.fit(X_train, y_train)\n",
    "#\n",
    "#dot_data = export_graphviz(DT, out_file=None, \n",
    "#                     feature_names=feature_reduced_drop.columns,  \n",
    "#                     class_names=feature_reduced['class'].unique(),  \n",
    "#                     filled=True, rounded=True,  \n",
    "#                     special_characters=True)  \n",
    "#\n",
    "#graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Tree\n",
    "\n",
    "An extratree is similar to a tree classifier except it more randomized and thusly produces more complex trees. An extratree tests a random number of splits rather than all splits. As we can see how just one tree looks, this is a very complex model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.tree import ExtraTreeClassifier\n",
    "#from sklearn.tree import export_graphviz\n",
    "#import graphviz\n",
    "#\n",
    "#tree = ExtraTreeClassifier(criterion='gini',\n",
    "#                              random_state=RANDOM_STATE,\n",
    "#                              max_depth = None,\n",
    "#                              max_features='auto')\n",
    "#\n",
    "#tree.fit(X_train, y_train)\n",
    "#dot_data = export_graphviz(tree, out_file=None, \n",
    "#                     feature_names=list(feature_reduced_drop.columns),  \n",
    "#                     class_names=['interictal', 'ictal'],  \n",
    "#                     filled=True, rounded=True,  \n",
    "#                     special_characters=True)  \n",
    "#\n",
    "#graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Averaging Methods\n",
    "Averaging methods build several separate estimators and then, as their name suggets, average their predictions. By reducing the variance these tend to perform better than any single base estimator<sup>1</sup>.\n",
    "\n",
    "## Bagging\n",
    "A bagging classifier is an ensemble of base classifiers, each fit on random subsets of a dataset. Their predictions are then pooled or aggregated to form a final prediction. This reduces variance of an estimator and can be a simple way to reduce overfitting. They work best with complex models as opposed to boosting, which work best with weak models<sup>1</sup>.\n",
    "\n",
    "Specifically, bagging is when sampling is produced with replacement<sup>2</sup>, and without replacement being called pasting<sup>3</sup>. Therefore both bagging and pasting allow training to be sampled several times across multipule predictors, with bagging only allowing several samples for the same predictor <sup>4</sup>.\n",
    "\n",
    "We can do this with any classifier so lets start with a support vector machine. \n",
    "\n",
    "**NOTE**\n",
    "- If we wanted to use pasting we would just set *bootstrap=False*.\n",
    "\n",
    "---\n",
    "\n",
    "1. https://scikit-learn.org/stable/modules/ensemble.html\n",
    "2. Breiman, L. (1996). Bagging predictors. Machine learning, 24(2), 123-140.\n",
    "3. Breiman, L. (1999). Pasting small votes for classification in large databases and on-line. Machine learning, 36(1-2), 85-103.\n",
    "4. Géron, A. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems. \" O'Reilly Media, Inc.\".\n",
    "\n",
    "---\n",
    "\n",
    "1. https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.ensemble import BaggingClassifier\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.metrics import classification_report\n",
    "#\n",
    "#pipe_svc = Pipeline([('scl', StandardScaler()),\n",
    "#                     ('pca', PCA(n_components=0.8, random_state = RANDOM_STATE)),\n",
    "#                     ('clf', SVC(kernel='rbf', random_state=RANDOM_STATE))])\n",
    "#\n",
    "#bag = BaggingClassifier(base_estimator=pipe_svc, \n",
    "#                        n_estimators=10, \n",
    "#                        max_samples=0.5, \n",
    "#                        max_features=0.5, \n",
    "#                        bootstrap=True, \n",
    "#                        bootstrap_features=True, \n",
    "#                        oob_score=True, \n",
    "#                        warm_start=False,\n",
    "#                        n_jobs=-1, \n",
    "#                        random_state=RANDOM_STATE)\n",
    "#bag.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional way we can get a performance metric on a validation set is to ensure we use `oob_score = True`\n",
    "\n",
    "With bagging by default only trains on a sample of the training data, leaving a set of training data sampled as out-of-bag (oob) instances. Since they are not seen during training, we can evalute on them without a separate validation using the oob_score.\n",
    "\n",
    "This estimated that its likely to get an accuracy of about 0.99 on the test/validation set so its a pretty close to what we did get above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Random forests are essentally bagged tree classifier. However, rather than using the bagging method above we can use one of the inbuilt methods Sklearn has specifically designed for fitting an ensemble of trees. A random forest is just a fancier version of bagging where multiple decision trees are averaged together to build a robust model that is less susceptible to overfitting.\n",
    "\n",
    "The random forest algorithm can be summarized in four steps<sup>1</sup>:\n",
    "\n",
    "> 1. *Draw a random bootstrap sample of size n (randomly choose n samples from the training set with replacement).*\n",
    "> 2. *Grow a decision tree from the bootstrap sample. At each node:*\n",
    ">\n",
    ">    *a. Randomly select d features without replacement.*\n",
    ">    \n",
    ">    *b. Split the node using the feature that provides the best split according to the objective function, for instance, maximizing the information gain.*\n",
    ">\n",
    ">3. *Repeat the steps 1-2 k times.*\n",
    ">4. *Aggregate the prediction by each tree to assign the class label by majority vote.*\n",
    "\n",
    "Instead of using majority vote, as was done in the origional publication<sup>2</sup>, in Sklearn the RandomForestClassifier averages the probabilistic prediction.\n",
    "\n",
    "**NOTES**\n",
    "- We cannot use the graphviz on the whole forest as we did for the trees in the supervised learning notebook, as each tree is built differently.\n",
    "\n",
    "---\n",
    "1. Raschka2017\n",
    "2. L. Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#\n",
    "#forest = RandomForestClassifier(criterion='gini',\n",
    "#                                n_estimators=1000,\n",
    "#                                max_features = 'sqrt',\n",
    "#                                class_weight = 'balanced',\n",
    "#                                random_state=RANDOM_STATE,\n",
    "#                                n_jobs=-1)\n",
    "#forest.fit(X_train, y_train)\n",
    "#\n",
    "#y_pred = forest.predict(X_val)\n",
    "#\n",
    "#display(pd.DataFrame(classification_report(y_val, y_pred , output_dict = True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at how a descion boundary created by a bagged tree could generalise better than a single tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mlxtend.plotting import plot_decision_regions\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#\n",
    "#tree = DecisionTreeClassifier(criterion='gini',\n",
    "#                              class_weight = 'balanced',\n",
    "#                              random_state=RANDOM_STATE)\n",
    "#\n",
    "#tree_dict = {'Tree':tree, 'Forest':forest}\n",
    "#\n",
    "#for classifier_name in tree_dict:\n",
    "#\n",
    "#  tree_dict[classifier_name].fit(two_features_data.values, reduced_features_reset['class'].values)\n",
    "#\n",
    "#  plot_decision_regions(two_features_data.values,\n",
    "#                        reduced_features_reset['class'].values,\n",
    "#                        clf = tree_dict[classifier_name])\n",
    "#\n",
    "#  plt.xlabel(x_axis_label) \n",
    "#  plt.ylabel(y_axis_label)\n",
    "#  \n",
    "#  print(color.BOLD+color.UNDERLINE+classifier_name+color.END)\n",
    "#  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as Random forests, Extremely randomized trees<sup>1</sup> can also be used. These work similarly to Random forests except...\n",
    "\n",
    "\"*...instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias*\"<sup>2</sup>.\n",
    "\n",
    "In this case it appears we are better served by the random forest.\n",
    "\n",
    "---\n",
    "1. Geurts, P., Ernst, D., & Wehenkel, L. (2006). Extremely randomized trees. Machine learning, 63(1), 3-42.\n",
    "2. https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction and RF\n",
    "### Model Stacking\n",
    "A method growing in popularity is to use model stacking, where the input to one model is the output of another. This allows for nonlinearities to be captured in the first model, and the potential to use a simple linear model as the last layer. Deep learning is an example of model stacking as, often neural networks are layered on top of one another, to optimize both the features and the classifier simultaneously<sup>1</sup>.\n",
    "\n",
    "---\n",
    "\n",
    "1. Zheng, A., & Casari, A. (2018). Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists. \" O'Reilly Media, Inc.\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Feature Importances with Random Forests\n",
    "\n",
    "An example of model stacking is to use the output of a decision tree–type model as input to a linear classifier. We can gain the importance for each feature by getting the average impurity decrease computed from all decision trees in the forest without regarding the linear separability of the classes. However, if features are highly correlated, one feature may be ranked highly while the information of the others not being fully captured<sup>1</sup>. \n",
    "\n",
    "This method has been previously used to find the best few EEG channels for absence seizure detection using the same spectral feature set in each channel of a 23 channel EEG system<sup>2</sup>. Indeed limited-channel EEG for seizure detection makes wearable EEG monitoring practical due to faster run time, lower power consumption, and increased accuracy by avoiding non-focal/unnecessary channels.\n",
    "\n",
    "Similar to Birjandtalab et al. (2017)<sup>2</sup> and Truong et al. (2017)<sup>3</sup>, we will do a random forest procedure on the EEG feature set to find the important features.\n",
    "\n",
    "---\n",
    "\n",
    "1. Raschka, Sebastian, and Vahid Mirjalili. Python Machine Learning, 2nd Ed. Packt Publishing, 2017.\n",
    "2. Birjandtalab, J., Pouyan, M. B., Cogan, D., Nourani, M., & Harvey, J. (2017). Automated seizure detection using limited-channel EEG and non-linear dimension reduction. Computers in biology and medicine, 82, 49-58.\n",
    "3.  Truong, N. D., Kuhlmann, L., Bonyadi, M. R., Yang, J., Faulks, A., & Kavehei, O. (2017). Supervised learning in automatic channel selection for epileptic seizure detection. Expert Systems with Applications, 86, 199–207. https://doi.org/10.1016/j.eswa.2017.05.055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#\n",
    "# create a forest classifier\n",
    "#forest = RandomForestClassifier(criterion='gini',\n",
    "#                                n_estimators=1000,\n",
    "#                                max_features = 'sqrt',\n",
    "#                                random_state=RANDOM_STATE,\n",
    "#                                n_jobs=-1)\n",
    "#\n",
    "# fit the classifier\n",
    "#forest.fit(X_train, y_train)\n",
    "#\n",
    "# get the importances for the features\n",
    "#importances = forest.feature_importances_\n",
    "#\n",
    "#importances_series = pd.Series(importances,index=feat_labels).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot the top 30 features to see how they contributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT_LIMIT = 30\n",
    "#\n",
    "# plot the important features\n",
    "#plt.figure(figsize=(15,10))\n",
    "#importances_series[:PLOT_LIMIT].plot.bar(legend =False, grid=False)\n",
    "#plt.title('Feature Importances ('+str(PLOT_LIMIT)+'/'+str(len(importances_series))+')')\n",
    "#\n",
    "#plt.xticks(rotation=45,ha='right')\n",
    "#plt.tight_layout()\n",
    "#\n",
    "#plt.savefig('forest_importances.png', dpi=300)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than manually setting a theshold like we have done (looking at the top 30) we can put it in a pipeline and use the SelectFromModel function from Scikit-learn. Using this we can still provide both a numeric theshold or we could use a heuristic such as the mean and median<sup>1</sup>.\n",
    "\n",
    "---\n",
    "\n",
    "1. http://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#from sklearn.feature_selection import SelectFromModel\n",
    "#from sklearn.svm import SVC\n",
    "#\n",
    "#clf = Pipeline([\n",
    "#  ('feature_selection', SelectFromModel(RandomForestClassifier(criterion='gini',\n",
    "#                                                               n_estimators=1000,\n",
    "#                                                               max_features = 'sqrt',\n",
    "#                                                               random_state=0,\n",
    "#                                                               n_jobs=-1), \n",
    "#                                        threshold = 'mean')),\n",
    "#  ('classification', SVC(kernel='rbf', random_state=RANDOM_STATE, class_weight = 'balanced'))\n",
    "#])\n",
    "#\n",
    "#scores = cross_val_score(estimator=pipe_svc, \n",
    "#                         X=X_train, \n",
    "#                         y=y_train, \n",
    "#                         scoring = 'f1',\n",
    "#                         cv=StratKFold,\n",
    "#                         n_jobs=-1)\n",
    "#\n",
    "#print('CV accuracy scores: %s' % scores)\n",
    "#print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imballanced Data and RF\n",
    "It is also worth noting we have been dealing with the class imballance found in this data by using `class_weight = 'balanced'` to assign more importance to getting ictal data predictions correct. We can however also undersample using a ballanced random forest. Generally what performs better depends on the amount of data you are training on. If small then class wight will be better (as seen below), but if you have very large datasets, then undersampling will likely work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weight = 'balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "#\n",
    "#bal_forest = BalancedRandomForestClassifier(criterion='gini',\n",
    "#                                            n_estimators=1000,\n",
    "#                                            max_features = 'sqrt',\n",
    "#                                            random_state=RANDOM_STATE,\n",
    "#                                            n_jobs=-1\n",
    "#                                            )\n",
    "#bal_forest.fit(X_train, y_train)\n",
    "#\n",
    "#y_pred = bal_forest.predict(X_val)\n",
    "#\n",
    "#display(pd.DataFrame(classification_report(y_val, y_pred , output_dict = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting (if Time)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyMNuZgSJ2BiDt5YMpOB66EK",
   "collapsed_sections": [],
   "name": "Clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
