{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Week 8 - Support Vector Machines\n",
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os # locating directories\n",
    "\n",
    "import numpy as np   # Arrays\n",
    "import pandas as pd  # DataFrames\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['animation.embed_limit'] = 30000000.0\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.datasets import load_iris           # for the Iris data\n",
    "from IPython.display import Image                # displaying .png images\n",
    "from sklearn.svm import SVC, LinearSVC           # SVM\n",
    "from mpl_toolkits.mplot3d import Axes3D          # 3d plots\n",
    "from sklearn.preprocessing import StandardScaler # scaling features\n",
    "from sklearn.preprocessing import LabelEncoder   # binary encoding\n",
    "\n",
    "# colours for print()\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "    \n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")\n",
    "    \n",
    "image_dir = os.path.join(os.getcwd(),\"Images\")\n",
    "\n",
    "iris = load_iris(as_frame=True)  # data stored in a `sklearn.utils.Bunch`\n",
    "iris_df = iris['data']           # get features DataFrame\n",
    "target = iris['target']          # get target Series\n",
    "# get the labels of flowers capitalised for visualisation\n",
    "target_names = list(map(lambda s: s.capitalize(), iris['target_names']))\n",
    "# create a dictionary with the original labels decoded (inverse of LabelEncoder)\n",
    "decode_label = dict(zip(range(3), target_names))\n",
    "# make a label encoder to use later if needed\n",
    "le = LabelEncoder().fit(target_names)\n",
    "# add the target labels to df for visualisation purposes\n",
    "iris_vis = pd.concat([iris_df, target],axis=1)\n",
    "# turn the ints to labels\n",
    "iris_vis[\"target\"] = iris_vis[\"target\"].replace(decode_label)\n",
    "# Capitalize column names for plotting\n",
    "iris_vis.columns = [x.capitalize() for x in list(iris_vis.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Question 1.\n",
    "What is the fundemental idea behind \"Maximal Margin Classifiers\" (as well as their extensions \"Support Vector Classifier\" and \"Support Vector Machines\")?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<details>\n",
    "<summary>Click here for answer</summary>\n",
    "The fundamental idea behind Maximal Margin Classifiers is to fit the widest possible margin between the classes. In other words, the goal is to have the largest possible \"street\" between the decision boundary that separates the two classes and the training instances.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question 2.\n",
    "What is a support vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<details>\n",
    "<summary>Click here for answer</summary>\n",
    "After training an SVM, a support vector is any instance located on the margin (see the previous answer), or between them when using soft margins (see later question). The decision boundary is entirely determined by the support vectors. Any instance that is not a support vector has no influence on the decision boundary. Computing the predictions only involves the support vectors, not the whole training set.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Question 3.\n",
    "In the plot below, which points are the \"support vectors\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Altered https://github.com/ageron/handson-ml2/blob/master/05_support_vector_machines.ipynb\n",
    "def svc_decision_boundary(clf, xmin=0, xmax=5.5, highlight=True, axes_limit = [0, 5.5, 0, 2]):\n",
    "    w = clf.coef_[0]\n",
    "    b = clf.intercept_[0]\n",
    "\n",
    "    # At the decision boundary, w0*x0 + w1*x1 + b = 0\n",
    "    # => x1 = -w0/w1 * x0 - b/w1\n",
    "    x0 = np.linspace(xmin, xmax, 200)\n",
    "    decision_boundary = -w[0]/w[1] * x0 - b/w[1]\n",
    "\n",
    "    margin = 1/w[1]\n",
    "    gutter_up = decision_boundary + margin\n",
    "    gutter_down = decision_boundary - margin\n",
    "\n",
    "    svs = clf.support_vectors_\n",
    "    if highlight:\n",
    "        g = sns.scatterplot(x = svs[:, 0], y = svs[:, 1], s=180, facecolors='#FFAAAA')\n",
    "    plt.plot(x0, decision_boundary, \"g-\", linewidth=2)\n",
    "    plt.plot(x0, gutter_up, \"r--\", linewidth=2)\n",
    "    plt.plot(x0, gutter_down, \"r--\", linewidth=2)\n",
    "    \n",
    "    plt.axis(axes_limit)\n",
    "\n",
    "\n",
    "exersise_ = iris_vis[[\"Sepal length (cm)\", \"Sepal width (cm)\", \"Target\"]]\n",
    "exersise_ = exersise_[exersise_.Target != \"Virginica\"]\n",
    "\n",
    "X = exersise_[[\"Sepal length (cm)\", \"Sepal width (cm)\"]].values\n",
    "y = le.transform(exersise_[[\"Target\"]].values.ravel())\n",
    "\n",
    "# SVM Classifier model\n",
    "svm_clf = SVC(kernel=\"linear\", C=float(\"inf\"))\n",
    "svm_clf.fit(X, y)\n",
    "w = svm_clf.coef_[0]\n",
    "b = svm_clf.intercept_[0]\n",
    "\n",
    "x0 = np.linspace(0, 5.5, 200)\n",
    "decision_boundary = -w[0]/w[1] * x0 - b/w[1]\n",
    "g = sns.scatterplot(data=exersise_, x = \"Sepal length (cm)\", \n",
    "                y = \"Sepal width (cm)\", hue=\"Target\", \n",
    "                style = \"Target\")\n",
    "g.axes.get_legend().set_title(False)\n",
    "plt.savefig('Images/Exercises/Question3q.png')\n",
    "\n",
    "svc_decision_boundary(svm_clf, 4.2, 7, axes_limit = [4.2, 7.2, 1.5, 5])\n",
    "g = sns.scatterplot(data=exersise_, x = \"Sepal length (cm)\", \n",
    "                y = \"Sepal width (cm)\", hue=\"Target\", \n",
    "                style = \"Target\")\n",
    "g.axes.get_legend().set_title(False)\n",
    "plt.savefig('Images/Exercises/Question3a.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/Exercises/Question3q.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<details>\n",
    "<summary>Click here for answer</summary>\n",
    "It uses more than 1 here for each class, although which ones are quite tricky to discern. Don't worry if you got a few of the points wrong here, the main thing is to note that sometimes it can be more than one point per class!\n",
    "<img src=\"Images/Exercises/Question3a.png\">\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Question 4.\n",
    "\n",
    "Sketch or code (using Python) the following two dimensional hyperplanes, indicating where $1 + 3X_1 - X_2 > 0$ and where $1 + 3X_1 - X_2 < 0$.\n",
    "\n",
    "a. $1 + 3X_1 - X_2 = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "x1 = np.linspace(-5.0, 5.0, 100)\n",
    "x2 = np.linspace(-5.0, 5.0, 100)\n",
    "X1, X2 = np.meshgrid(x1,x2)\n",
    "a = 1 + 3*X1 -X2\n",
    "plt.contour(X1, X2, a, [0])\n",
    "plt.text(-4, 0, \"$1 + 3X_1 - X_2 < 0$\")\n",
    "plt.text(1, 0, \" $1 + 3X_1 - X_2 > 0$\")\n",
    "\n",
    "plt.suptitle(\"4a. $1 + 3X_1 - X_2 = 0$\")\n",
    "\n",
    "plt.savefig('Images/Exercises/Question4a.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for answer</summary>\n",
    "<img src=\"Images/Exercises/Question4a.png\">\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. $-2 + X_1 + 2X_2 = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "x1 = np.linspace(-5.0, 5.0, 100)\n",
    "x2 = np.linspace(-5.0, 5.0, 100)\n",
    "X1, X2 = np.meshgrid(x1,x2)\n",
    "b = -2 + X1 + 2*X2\n",
    "plt.contour(X1, X2, b, [0])\n",
    "plt.text(-4, -2, \"$-2 + X_1 + 2X_2 < 0$\")\n",
    "plt.text(1, 2, \" $-2 + X_1 + 2X_2 > 0$\")\n",
    "\n",
    "plt.suptitle(\"4b. $-2 + X_1 + 2X_2 = 0$\")\n",
    "\n",
    "plt.savefig('Images/Exercises/Question4b.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for answer</summary>\n",
    "<img src=\"Images/Exercises/Question4b.png\">\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Question 5. \n",
    "\n",
    "Fundamentally, how are \"Support Vector Classifier\" and \"Support Vector Machines\" extensions of \"Maximal Margin Classifiers\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<details>\n",
    "<summary>Click here for answer</summary>\n",
    "When using a Support Vector Classifier (or soft margin classification), the SVC searches for a compromise between perfectly separating the two classes and having the widest possible street (i.e., a few instances may end up on the street). \n",
    "<br><br>\n",
    "Support Vector Machines use kernels when training on nonlinear datasets.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Question 6. \n",
    "\n",
    "If $C$ is large for a support vector classifier in Scikit-Learn, will there be more or less support vectors than if $C$ is small? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for answer</summary>\n",
    "When the tuning parameter $C$ is large in Scikit-Learn, then there are less support vectors, meaning fewer observations are involved in determining the hyperplane. The strength of the regularization is inversely proportional to $C$, meaning a large $C$ has a higher penelty.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Question 7.\n",
    "Is the \"confidence score\" output from a SVM classifier the same as a \"probability score\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for answer</summary>\n",
    "No, the output of the SVM, the distance between the test instance and the decision boundary, cannot be directly converted into an estimation of the class probability. \n",
    "<br><br>\n",
    "<b>Note</b>\n",
    "If you set <code>probability=True</code> when creating an SVM in Scikit-Learn, then after training it will calibrate the probabilities using Logistic Regression on the SVMâ€™s scores (trained by an additional five-fold cross-validation on the training data). This will add the <code>predict_proba()</code> and <code>predict_log_proba()</code> methods to the SVM.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Question 8.\n",
    "Say you trained an SVM classifier with an RBF kernel. It seems to underfit the training set: should you increase or decrease $\\gamma$ (`gamma`) and/or $C$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for answer</summary>\n",
    "If an SVM classifier trained with an RBF kernel underfits the training set, there might be too much regularization. To decrease it, you need to increase <code>gamma</code> or <code>C</code> (or both).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook SVM_Exercises.ipynb to html\n"
     ]
    }
   ],
   "source": [
    "# Create HTML document\n",
    "!jupyter nbconvert SVM_Exercises.ipynb \\\n",
    "    --to html \\\n",
    "    --output-dir . \\\n",
    "    --template classic \\\n",
    "    --TemplateExporter.exclude_input=True \\\n",
    "    --TemplateExporter.exclude_output=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "colab": {
   "authorship_tag": "ABX9TyMNuZgSJ2BiDt5YMpOB66EK",
   "collapsed_sections": [],
   "name": "Clustering.ipynb",
   "provenance": []
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
